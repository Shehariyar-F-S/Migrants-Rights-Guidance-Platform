# Migrants-Rights-Guidance-Platform

A local Retrievalâ€‘Augmented Generation (RAG) assistant that answers questions about migrantsâ€™ rights and procedures using official legal PDFs (e.g. the Dublin Regulation) as its only knowledge source.
The system runs entirely on a laptop using openâ€‘source tools (Ollama + Llama 3, ChromaDB, LangChain, Streamlit).

ğŸš€ Features
Domainâ€‘specific legal assistant for EU asylum/migration documents (starting with the Dublin Regulation).

Localâ€‘only stack: all documents, embeddings, and LLM inference run on your machine.

Multiâ€‘PDF support: drop multiple PDFs into a folder and index them together.

RAG pipeline:

PDF â†’ text extraction â†’ chunking

Sentenceâ€‘Transformer embeddings â†’ Chroma vector store

Topâ€‘k retrieval â†’ Llama 3 via Ollama for grounded answers

Two interfaces:

CLI Q&A (terminal)

Web chat UI built with Streamlit

ğŸ§± Architecture
Indexing step (offline)

Load all PDFs from data/ (e.g. DUBLIN REGULATIONS.pdf and other guides).

Split pages into overlapping chunks.

Create embeddings with a Sentenceâ€‘Transformer model (e.g. all-MiniLM-L6-v2).

Store embeddings, text, and metadata (including source filename) in a persistent ChromaDB collection (e.g. dublin_chroma).

Retrieval & generation (online)

For each user question:

Embed the question with the same model.

Retrieve topâ€‘k similar chunks from Chroma.

Build a legalâ€‘style prompt: â€œAnswer only from this context; if not present, say you donâ€™t know.â€

Call Llama 3 running locally via Ollama to generate a concise answer.

Optionally display which PDF each answer is grounded in.

User interfaces

CLI: ask questions in the terminal, get a text answer.

Web UI (Streamlit): chatâ€‘style interface with history, suitable for demoing in a browser.

ğŸ“‚ Project structure
Example layout (simplify/adjust if your repo differs):

text
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ DUBLIN REGULATIONS.pdf
â”‚   â””â”€â”€ other_documents.pdf
â”œâ”€â”€ db/                      # Chroma persistent data (autoâ€‘created)
â”œâ”€â”€ index_pdfs.py            # Oneâ€‘time (or occasional) indexing script
â”œâ”€â”€ query_cli.py             # CLI questionâ€‘answer interface
â”œâ”€â”€ dublin_rag.py            # Core RAG logic (rag_answer function)
â”œâ”€â”€ app.py                   # Streamlit web UI
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
ğŸ”§ Setup
Clone the repository

bash
git clone https://github.com/<your-username>/migrants-rights-guidance-platform.git
cd migrants-rights-guidance-platform
Create and activate a virtual environment (optional but recommended)

bash
python -m venv .venv
source .venv/bin/activate  # on Windows: .venv\Scripts\activate
Install dependencies

bash
pip install -r requirements.txt
A typical requirements.txt for this project might include:

text
langchain
langchain-community
chromadb
sentence-transformers
pypdf
streamlit
ollama
Install and set up Ollama

Install Ollama from its official site.

Pull the Llama 3 model:

bash
ollama pull llama3
Make sure running this works:

bash
ollama run llama3
Add your PDFs

Place your legal PDFs in the data/ folder:

text
data/
 â”œâ”€â”€ DUBLIN REGULATIONS.pdf
 â”œâ”€â”€ guide_1.pdf
 â””â”€â”€ guide_2.pdf
ğŸ§® Step 1 â€“ Index the documents
Run the indexing script (once or whenever you add/change PDFs):

bash
python index_pdfs.py
What this does:

Iterates over all *.pdf in data/.

Extracts text and splits into chunks.

Embeds chunks with all-MiniLM-L6-v2.

Stores embeddings + text in a ChromaDB collection under db/.

ğŸ’¬ Step 2 â€“ Ask questions (CLI)
Use the terminal interface:

bash
python query_cli.py
Example interaction:

text
Your question (or 'exit'): Who is responsible for examining an asylum application under the Dublin Regulation?

Answer:
<model answer here based on retrieved chunks>

Your question (or 'exit'):
ğŸŒ Step 3 â€“ Web UI (Streamlit)
Start the Streamlit app:

bash
streamlit run app.py
Open the URL shown in the terminal (usually http://localhost:8501).

In the browser you can:

Ask questions about the Dublin Regulation and related documents.

See answers generated by Llama 3, grounded in the indexed PDFs.

Optionally see which source document each answer used.

ğŸ” Example questions
Some example questions you can try:

â€œWhich country is responsible for examining my asylum application under the Dublin rules?â€

â€œWhat happens if my fingerprints were taken in another EU country?â€

â€œHow are family members treated in the Dublin procedure?â€

â€œAre there special rules for unaccompanied minors?â€

ğŸ§± Tech stack
Language model: Llama 3 (via Ollama, running locally)

Framework & orchestration: LangChain

Vector database: ChromaDB (persistent, local)

Embeddings: Sentenceâ€‘Transformers (all-MiniLM-L6-v2)

PDF processing: PyPDF or LangChainâ€™s PyPDFLoader

UI: Streamlit (web), simple Python CLI (terminal)

ğŸŒ± Future work
Add multilingual support (e.g. English/German answers).

Expose articleâ€‘level citations and clickable references back into PDFs.

Extend beyond Dublin to broader EU and national migration law.

Add authentication and roleâ€‘based access for NGOs or caseworkers.

ğŸ¤ Contributing
Pull requests and issues are welcome.
Ideas: better prompts, additional document collections, improved UI/UX, evaluation scripts for answer quality.

